{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting training data...\n",
      "Please record 5 samples of the target voice saying anything.\n",
      "Recording for 10 seconds...\n",
      "Recording complete.\n",
      "Recording for 10 seconds...\n",
      "Recording complete.\n",
      "Recording for 10 seconds...\n",
      "Recording complete.\n",
      "Recording for 10 seconds...\n",
      "Recording complete.\n",
      "Recording for 10 seconds...\n",
      "Recording complete.\n",
      "Now, please record 5 samples of other voices saying anything.\n",
      "Recording for 10 seconds...\n",
      "Recording complete.\n",
      "Recording for 10 seconds...\n",
      "Recording complete.\n",
      "Recording for 10 seconds...\n",
      "Recording complete.\n",
      "Recording for 10 seconds...\n",
      "Recording complete.\n",
      "Recording for 10 seconds...\n",
      "Recording complete.\n",
      "Training the voice recognition model...\n",
      "\n",
      "Now, let's test the system. Please say: 'Hello world'\n",
      "Recording for 10 seconds...\n",
      "Recording complete.\n",
      "Voice recognition distance: 0.5636\n",
      "Transcribed text: OW WE EXPRESS OURSELF CAN MAKE IT DIFFICULT FOR US NORFO E MUST MEAN TO UNDERSTAND US PROWL FRECTLY BUT WI ON GOING ROM AND THESE FELLERS HARAD WE ON\n",
      "Sentence similarity: 0.0000\n",
      "Recognition result: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Constants\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 10\n",
    "\n",
    "# Load pre-trained model and processor\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "\n",
    "def record_audio(duration, sample_rate):\n",
    "    print(f\"Recording for {duration} seconds...\")\n",
    "    audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype='float32')\n",
    "    sd.wait()\n",
    "    print(\"Recording complete.\")\n",
    "    return audio.flatten()\n",
    "\n",
    "def process_audio(audio):\n",
    "    input_values = processor(audio, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\").input_values\n",
    "    return input_values\n",
    "\n",
    "def extract_features(audio):\n",
    "    input_values = process_audio(audio)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values)\n",
    "    return outputs.logits.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "def speech_to_text(audio):\n",
    "    input_values = process_audio(audio)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "    return transcription\n",
    "\n",
    "def train_voice_model(target_voice_samples, other_voice_samples):\n",
    "    target_features = np.array(target_voice_samples)\n",
    "    other_features = np.array(other_voice_samples)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    all_features = np.vstack([target_features, other_features])\n",
    "    scaler.fit(all_features)\n",
    "    \n",
    "    target_centroid = np.mean(scaler.transform(target_features), axis=0)\n",
    "    return target_centroid, scaler\n",
    "\n",
    "def recognize_voice(features, target_centroid, scaler, threshold=0.5):\n",
    "    scaled_features = scaler.transform(features.reshape(1, -1))\n",
    "    distance = cosine(scaled_features.squeeze(), target_centroid)\n",
    "    return distance < threshold, distance\n",
    "\n",
    "def sentence_match(transcribed_text, target_sentence, threshold=0.7):\n",
    "    transcribed_words = set(transcribed_text.lower().split())\n",
    "    target_words = set(target_sentence.lower().split())\n",
    "    similarity = len(transcribed_words.intersection(target_words)) / len(target_words)\n",
    "    return similarity >= threshold, similarity\n",
    "\n",
    "def recognize_voice_and_sentence(audio, target_centroid, scaler, target_sentence):\n",
    "    features = extract_features(audio)\n",
    "    is_target_voice, voice_distance = recognize_voice(features, target_centroid, scaler)\n",
    "    print(f\"Voice recognition distance: {voice_distance:.4f}\")\n",
    "    \n",
    "    transcribed_text = speech_to_text(audio)\n",
    "    print(f\"Transcribed text: {transcribed_text}\")\n",
    "    \n",
    "    is_target_sentence, sentence_similarity = sentence_match(transcribed_text, target_sentence)\n",
    "    print(f\"Sentence similarity: {sentence_similarity:.4f}\")\n",
    "    \n",
    "    return is_target_voice and is_target_sentence\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    target_sentence = \"Hello world\"\n",
    "    \n",
    "    print(\"Collecting training data...\")\n",
    "    print(\"Please record 5 samples of the target voice saying anything.\")\n",
    "    target_voice_samples = [extract_features(record_audio(DURATION, SAMPLE_RATE)) for _ in range(5)]\n",
    "    \n",
    "    print(\"Now, please record 5 samples of other voices saying anything.\")\n",
    "    other_voice_samples = [extract_features(record_audio(DURATION, SAMPLE_RATE)) for _ in range(5)]\n",
    "    \n",
    "    print(\"Training the voice recognition model...\")\n",
    "    target_centroid, scaler = train_voice_model(target_voice_samples, other_voice_samples)\n",
    "    \n",
    "    print(f\"\\nNow, let's test the system. Please say: '{target_sentence}'\")\n",
    "    test_audio = record_audio(DURATION, SAMPLE_RATE)\n",
    "    \n",
    "    result = recognize_voice_and_sentence(test_audio, target_centroid, scaler, target_sentence)\n",
    "    print(f\"Recognition result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now, let's test the system. Please say: 'Hello world'\n",
      "Recording for 10 seconds...\n",
      "Recording complete.\n",
      "Voice recognition distance: 1.0674\n",
      "Transcribed text: SO THIS IS HALLO WALD HALLO WARL HALLO WARLD HALLO WARLD HALLO WARLD HALLO WARD S\n",
      "Sentence similarity: 0.0000\n",
      "Recognition result: False\n"
     ]
    }
   ],
   "source": [
    "    print(f\"\\nNow, let's test the system. Please say: '{target_sentence}'\")\n",
    "    test_audio = record_audio(DURATION, SAMPLE_RATE)\n",
    "    \n",
    "    result = recognize_voice_and_sentence(test_audio, target_centroid, scaler, target_sentence)\n",
    "    print(f\"Recognition result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TextS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
